"""
link_cleaner_v2.py

Purpose:
- Take the raw crawler output from mdc_links_raw_v2.txt
  (one URL per line, generated by crawler.py)
- Normalize and clean the URLs
- Remove duplicates
- Optionally filter out "junk" URLs (logins, confirms, etc.)
- Write a clean, de-duplicated list of URLs, one per line,
  to cleaned_links_v2.txt
"""

import os
from urllib.parse import urldefrag, urlparse

# --------- CONFIG ---------

INPUT_RAW_PATH = "../data/mdc_links_raw_v2.txt"
OUTPUT_CLEAN_PATH = "../data/cleaned_links_v2.txt"

ALLOWED_DOMAIN_SUFFIX = "mdc.edu"

# Anything you *never* want to keep can go here
SKIP_EXTENSIONS = (
    ".pdf", ".jpg", ".jpeg", ".png", ".gif",
    ".doc", ".docx", ".xls", ".xlsx",
    ".ppt", ".pptx", ".zip", ".rar",
    ".mp4", ".mp3",
)

# --------- HELPERS ---------

def normalize_url(url: str) -> str:
    """
    Basic normalization:
    - strip whitespace
    - remove URL fragment (#something)
    """
    url = url.strip()
    if not url:
        return ""
    clean, _ = urldefrag(url)
    return clean

def has_skipped_extension(url: str) -> bool:
    path = urlparse(url).path.lower()
    return path.endswith(SKIP_EXTENSIONS)

def is_junk_url(url: str) -> bool:
    """
    Extra filters for URLs we know we don't want in the clean list.
    Adjust this as you learn more about MDC's patterns.
    """
    if not url:
        return True

    parsed = urlparse(url)

    # keep only http(s)
    if parsed.scheme not in ("http", "https"):
        return True

    # stay inside MDC
    if not parsed.netloc.endswith(ALLOWED_DOMAIN_SUFFIX):
        return True

    # skip obvious login / confirm / auth flows
    full = url.lower()
    if "auth/shib_login" in full:
        return True
    if "calendar.mdc.edu/event/" in full and "confirm" in full:
        return True

    # skip non-HTML resources
    if has_skipped_extension(url):
        return True

    return False

# --------- MAIN CLEAN STEP ---------

def load_raw_urls(path: str):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Input file not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return [line.rstrip("\n") for line in f if line.strip()]

def clean_urls(raw_urls):
    seen = set()
    cleaned = []

    for url in raw_urls:
        norm = normalize_url(url)
        if not norm:
            continue
        if is_junk_url(norm):
            continue
        if norm in seen:
            continue
        seen.add(norm)
        cleaned.append(norm)

    return cleaned

def write_urls(urls, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")

# --------- SCRIPT ENTRY POINT ---------

if __name__ == "__main__":
    print(f"Loading raw URLs from {INPUT_RAW_PATH} ...")
    raw_urls = load_raw_urls(INPUT_RAW_PATH)
    print(f"  Raw URLs: {len(raw_urls)}")

    cleaned = clean_urls(raw_urls)
    print(f"  Cleaned & unique URLs: {len(cleaned)}")

    write_urls(cleaned, OUTPUT_CLEAN_PATH)
    print(f"Saved cleaned URLs to {OUTPUT_CLEAN_PATH}")
