"""
link_cleaner.py (v3)

Purpose:
- Take the raw crawler output from mdc_links_raw_v3.txt
  (one URL per line, generated by crawler.py)
- Normalize and clean the URLs
- Remove duplicates
- Filter out "junk" URLs (logins, confirms, off-domain, etc.)
- Split into:
    * HTML-like URLs
    * PDF URLs (handled separately later)
- Write two clean, de-duplicated lists of URLs, one per line:
    - html_links_v3.txt
    - pdf_links_v3.txt
"""

import os
from urllib.parse import urldefrag, urlparse

# --------- CONFIG ---------

INPUT_RAW_PATH = "../data/mdc_links_raw_v3.txt"
OUTPUT_HTML_PATH = "../data/html_links_v3.txt"
OUTPUT_PDF_PATH = "../data/pdf_links_v3.txt"

ALLOWED_DOMAIN_SUFFIX = "mdc.edu"

# Anything you *never* want to keep can go here
# v3 removed pdf from list of exceptions so we KEEP pdf URLs
SKIP_EXTENSIONS = (
    # ".pdf",
    ".jpg", ".jpeg", ".png", ".gif",
    ".doc", ".docx", ".xls", ".xlsx",
    ".ppt", ".pptx", ".zip", ".rar",
    ".mp4", ".mp3",
)

# --------- HELPERS ---------

def normalize_url(url: str) -> str:
    """
    Basic normalization:
    - strip whitespace
    - remove URL fragment (#something)
    """
    url = url.strip()
    if not url:
        return ""
    clean, _ = urldefrag(url)
    return clean


# v3 add get_extension function
def get_extension(url: str) -> str:
    """
    Return the lowercase file extension (e.g. ".pdf") or "" if none.
    """
    path = urlparse(url).path.lower()
    filename = path.rsplit("/", 1)[-1]
    if "." in filename:
        return "." + filename.rsplit(".", 1)[-1]
    return ""


# v3 update, no longer used
# def has_skipped_extension(url: str) -> bool:
#     path = urlparse(url).path.lower()
#     return path.endswith(SKIP_EXTENSIONS)


def is_junk_url(url: str) -> bool:
    """
    Extra filters for URLs we know we don't want in the clean list.
    Adjust this as you learn more about MDC's patterns.
    """
    if not url:
        return True

    parsed = urlparse(url)

    # keep only http(s)
    if parsed.scheme not in ("http", "https"):
        return True

    # stay inside MDC
    if not parsed.netloc.endswith(ALLOWED_DOMAIN_SUFFIX):
        return True

    # skip obvious login / confirm / auth flows
    full = url.lower()
    if "auth/shib_login" in full:
        return True
    if "calendar.mdc.edu/event/" in full and "confirm" in full:
        return True

    # skip non-HTML *non-PDF* resources
    ext = get_extension(url)
    if ext in SKIP_EXTENSIONS:
        return True

    return False


# --------- MAIN CLEAN STEP ---------

def load_raw_urls(path: str):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Input file not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return [line.rstrip("\n") for line in f if line.strip()]


def clean_and_split_urls(raw_urls):
    """
    v3:
    - normalize
    - filter junk
    - de-duplicate
    - split into HTML-like URLs and PDF URLs
    """
    seen = set()
    html_urls = []
    pdf_urls = []

    for url in raw_urls:
        norm = normalize_url(url)
        if not norm:
            continue
        if is_junk_url(norm):
            continue
        if norm in seen:
            continue
        seen.add(norm)

        ext = get_extension(norm)
        if ext == ".pdf":
            pdf_urls.append(norm)
        else:
            html_urls.append(norm)

    return html_urls, pdf_urls


def write_urls(urls, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        for url in urls:
            f.write(url + "\n")


# --------- SCRIPT ENTRY POINT ---------

if __name__ == "__main__":
    print(f"Loading raw URLs from {INPUT_RAW_PATH} ...")
    raw_urls = load_raw_urls(INPUT_RAW_PATH)
    print(f"  Raw URLs: {len(raw_urls)}")

    html_urls, pdf_urls = clean_and_split_urls(raw_urls)
    print(f"  Cleaned & unique HTML-like URLs: {len(html_urls)}")
    print(f"  Cleaned & unique PDF URLs:       {len(pdf_urls)}")

    write_urls(html_urls, OUTPUT_HTML_PATH)
    write_urls(pdf_urls, OUTPUT_PDF_PATH)

    print(f"Saved HTML URLs to {OUTPUT_HTML_PATH}")
    print(f"Saved PDF URLs  to {OUTPUT_PDF_PATH}")
