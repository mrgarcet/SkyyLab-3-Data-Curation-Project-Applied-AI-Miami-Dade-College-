# ğŸ§© MDC Data Curation & Web Scraping Pipeline
### *Miami Dade College â€“ Applied AI / NLP Project*

**Author:** `Garcet, Lorenzo A.`  
**Team Members:** `Duran, Fabrizio Andres` Â· `Lopez Jr., Jorge` Â· `Martinez III, Miguel Angel`

**Component Versions (from source):**
- `pipeline/crawler.py` â€” **v0.2.1**
- `pipeline/link_cleaner.py` â€” **v0.2.1**
- `pipeline/url_categorizer.py` â€” **v0.2.2**
- `scraper/page_scrapper.py` (file name) / â€œpage_scraper.pyâ€ (module title) â€” **v1.0.0**
- `scraper/pdf_scraper.py` â€” **v1.0.0**
- Legacy tools (in `/legacy`) â€” historical utilities provided by MDC

**Data Snapshot Label:** **v3** (matches current file names in `/data`)  
**Last Updated:** **2025â€‘11â€‘17**

---

## ğŸ“˜ Overview

This repository implements a **repeatable, resilient pipeline** to build an AIâ€‘ready knowledge base from the public website of **Miami Dade College (mdc.edu)**.

Instead of a rigid, pageâ€‘structureâ€‘dependent scraper, we intentionally split the process into **independent stages**:

1. **Collect** â€” crawl and record *all* discoverable onâ€‘domain URLs (including PDFs).
2. **Clean** â€” normalize URLs, remove duplicates and junk, and split **HTML vs PDF**.
3. **Categorize** â€” tag URLs by topic (e.g., Admissions, Financial Aid) to support targeted curation and scrape priority.
4. **Extract** â€” scrape structured content from HTML pages and text from PDFs into **lineâ€‘delimited JSON**.

> **Why this design?**  
> MDCâ€™s site changes often. By **decoupling** discovery, cleaning, categorization, and extraction, the pipeline remains **adaptable** if pages or layouts change. Each stage can be **reâ€‘run periodically** (e.g., on a schedule), and outputs are versioned (current **v3**) to keep historical snapshots. The result is a **scalable** system that wonâ€™t break on minor HTML changes and that supports incremental refreshes.

---

## âš™ï¸ Highâ€‘Level Workflow

1. `pipeline/crawler.py` â†’ `data/mdc_links_raw_v3.txt` (+ `data/mdc_crawler_errors_v3.log`)  
2. `pipeline/link_cleaner.py` â†’ `data/html_links_v3.txt` and `data/pdf_links_v3.txt`  
3. `pipeline/url_categorizer.py`  
   - HTML: â†’ `data/urls_with_category_v3.csv` + **targets** â†’ `data/target_links_v3.txt`  
   - PDF (optional): â†’ `data/pdfs_with_category_v3.csv`  
4. `scraper/page_scrapper.py` â†’ `data/mdc_pages_v3.jsonl`  
5. `scraper/pdf_scraper.py` â†’ `data/mdc_pdfs_v3.jsonl`

All JSON outputs are **lineâ€‘delimited** (one JSON object per line) and safe to append for resumable runs.

---

## ğŸ“‚ Repository Contents
```
**Skyy
Â¦   README.md
Â¦   requirements.txt
Â¦               
+---aged_data_files
Â¦       cleaned_links_v2.txt
Â¦       mdc_crawler_errors.log
Â¦       mdc_crawler_errors_v2.log
Â¦       mdc_links_raw.txt
Â¦       mdc_links_raw_v2.txt
Â¦       mdc_pages_v3.jsonl
Â¦       target_links_v1.txt
Â¦       urls_with_category_v1.csv
Â¦       
+---data
Â¦       html_links_v3.txt
Â¦       mdc_crawler_errors_v3.log
Â¦       mdc_links_raw_v3.txt
Â¦       mdc_pages_v3.jsonl
Â¦       mdc_pdfs_v3.jsonl
Â¦       pdfs_with_category_v3.csv
Â¦       pdf_links_v3.txt
Â¦       target_links_v3.txt
Â¦       urls_with_category_v3.csv
Â¦       
+---docs
Â¦       changelog.md
Â¦       
+---legacy
Â¦   Â¦   legacy__README.md
Â¦   Â¦   
Â¦   +---data
Â¦   Â¦       legacy_cleaned_links_set.txt
Â¦   Â¦       legacy_no_pdf_list.txt
Â¦   Â¦       legacy_no_pdf_list_div1.txt
Â¦   Â¦       legacy_no_pdf_list_div2.txt
Â¦   Â¦       legacy_no_pdf_list_div3.txt
Â¦   Â¦       legacy_processed_mdc_links.txt
Â¦   Â¦       
Â¦   +---scrapper
Â¦   Â¦       legacy_link_cleaner.py
Â¦   Â¦       legacy_link_cleaner_v0.0.2.py
Â¦   Â¦       legacy_list_divider.py
Â¦   Â¦       legacy_pdf_remover.py
Â¦   Â¦       legacy_p_tag_scrapper.py
Â¦   Â¦       
Â¦   +---__MACOSX
Â¦       +---data
Â¦       Â¦       ._cleaned_links_set.txt
Â¦       Â¦       ._processed_mdc_links.txt
Â¦       Â¦       
Â¦       +---scrapper
Â¦               ._link_cleaner.py
Â¦               ._link_cleaner_v0.0.2.py
Â¦               ._p_tag_scrapper.py
Â¦               
+---pipeline
Â¦       crawler.py
Â¦       link_cleaner.py
Â¦       url_categorizer.py
Â¦       
+---scraper
        page_scrapper.py
        pdf_scraper.py
```

---

**What lives where (quick guide):**
- **`/pipeline`** â€” modern pipeline components:
  - `crawler.py` (v0.2.1) â€” onâ€‘domain URL discovery, error logging
  - `link_cleaner.py` (v0.2.1) â€” normalization, deâ€‘dup, split HTML/PDF
  - `url_categorizer.py` (v0.2.2) â€” ruleâ€‘based topic labeling + target list
- **`/scraper`** â€” content extraction:
  - `page_scrapper.py` (v1.0.0) â€” HTML page parsing â†’ `mdc_pages_v3.jsonl`
  - `pdf_scraper.py` (v1.0.0) â€” PDF fetch + text extraction â†’ `mdc_pdfs_v3.jsonl`
- **`/data`** â€” **current v3** inputs/outputs for the latest run(s)
- **`/aged_data_files`** â€” historical artifacts from earlier versions (v1/v2)
- **`/legacy`** â€” original MDC utilities (kept for reference/backward compatibility)
- **`/docs/changelog.md`** â€” incremental notes and version bumps (author updates)

> âœ… The pipeline scripts default to reading/writing relative to `../data/` from their own folder, which matches this layout.

---

## ğŸš€ Quick Start

This project is designed to run as a **repeatable pipeline** so you can reâ€‘crawl and reâ€‘ingest MDC (mdc.edu) content as the website evolves without brittle assumptions. The flow is: **crawl â†’ clean â†’ categorize â†’ scrape HTML â†’ scrape PDFs**. Defaults are already wired to the `data/` folder.

### 1) Prereqs
- **Python 3.11+** (recommended; code uses modern `datetime.UTC` and 3.12â€‘safe patterns)
- **pip** available on PATH

### 2) Create & activate a virtual environment
**Windows (PowerShell)**
```powershell
cd C:\Skyy
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

### 3) Install Dependencies
`pip install -r requirements.txt`

### 4) Run the pipeline  
All scripts have sensible defaults and write into the data/ directory:  
`python pipeline/crawler.py`


---
# Run the pipeline (endâ€‘toâ€‘end):
## 1) Crawl
python pipeline/crawler.py

## 2) Clean and split HTML/PDF
python pipeline/link_cleaner.py

## 3) Categorize (HTML required; PDF optional)
python pipeline/url_categorizer.py \
  --input ../data/html_links_v3.txt \
  --out-csv ../data/urls_with_category_v3.csv \
  --out-txt ../data/target_links_v3.txt \
  --pdf ../data/pdf_links_v3.txt \
  --pdf-out-csv ../data/pdfs_with_category_v3.csv

## 4) Scrape HTML pages (MVP categories)
python scraper/page_scrapper.py \
  --in_urls ../data/target_links_v3.txt \
  --in_labels ../data/urls_with_category_v3.csv \
  --out_jsonl ../data/mdc_pages_v3.jsonl

## 5) Scrape PDFs (optional, recommended)
python scraper/pdf_scraper.py \
  --in_pdfs ../data/pdf_links_v3.txt \
  --in_labels ../data/pdfs_with_category_v3.csv \
  --out_jsonl ../data/mdc_pdfs_v3.jsonl

---
# ğŸ§© Detailed Script Documentation
1) `pipeline/crawler.py` â€” v0.2.1

## **Purpose**  
Onâ€‘domain crawler for `mdc.edu`. Starts at `https://www.mdc.edu/`, follows internal links, and emits one URL per line
for downstream processing.

**Key defaults & paths**  
- Seeds: `["https://www.mdc.edu/"]`  
- Domain scope: `mdc.edu`  
- Max pages: `DEFAULT_MAX_PAGES = 20000`  
- Politeness: `REQUEST_DELAY = 1.0s, REQUEST_TIMEOUT = 20s`    
- Output: `../data/mdc_links_raw_v3.txt`  
- Errors: `../data/mdc_crawler_errors_v3.log`

- Skips (examples):  
  - **Disallowed prefixes:** `/newsandnotes/`  `/trackback/` `/publications/` `/email/`
  - **Extensions:** images, office docs, archives, media; PDFs are allowed in v3  
  - **Known junk:** calendar event confirmation links; generic Shibboleth/auth endpoints

## Behavior

- Normalizes URLs (drops fragments).  
- Deâ€‘dupes during crawl.
- Writes all discovered URLs to the raw list upon completion. 

---
2) `pipeline/link_cleaner.py` â€” v0.2.1

# Purpose
Consume `mdc_links_raw_v3.txt` and produce two deâ€‘duplicated, normalized lists:
- `../data/html_links_v3.txt`
- `../data/pdf_links_v3.txt`

## What it does
- Trims whitespace and fragments.
- Removes clear junk (logins, confirms, offâ€‘domain).
- Keeps `.pdf` URLs by design (handled separately).
- Splits by extension using a simple `get_extension()` helper.

## I/O (fixed paths)
- In: `../data/mdc_links_raw_v3.txt ` 
- Out: `../data/html_links_v3.txt`, `../data/pdf_links_v3.txt`

Runs as a script; no CLI flags in this version.

---
3) `pipeline/url_categorizer.py` â€” v0.2.2

# Purpose
Ruleâ€‘based categorization of URLs to support targeted scraping and downstream curation.

## Inputs / Outputs (defaults)
- HTML
  - In: `../data/html_links_v3.txt`
  - Out: `../data/urls_with_category_v3.csv` (columns: url, category, confidence, reason)
  - Targets (MVP only): `../data/target_links_v3.txt`
- PDF (optional)
  - In: `../data/pdf_links_v3.txt`
  - Out: `../data/pdfs_with_category_v3.csv`
  - Optional targets: `--pdf-out-txt <path>`

## MVP Categories (builtâ€‘in set)
- Admissions & Getting Started
- Advising & Registration
- Testing & Placement
- Costs & Payments
- Financial Aid & Scholarships
- Programs, Degrees & Catalog
- Student Resources & Support
- Library & Research
- Career Services (MDC WORKS)
- Continuing Education (non-credit)
- MDC Online
- International Students
- Veterans & Military
- Campuses & Locations

## How it works
- Combines pathâ€‘regex and keyword heuristics to propose categories.
- Scores candidates and prefers MVP categories on ties.
- Writes a labeled CSV including a confidence and reason field.
- Writes target_links_v3.txt with HTML links in MVP categories for the HTML scraper.

---
4) `scraper/page_scrapper.py` â€” v1.0.0
(File name uses â€œscrapperâ€; internal module title says â€œpage_scraper.pyâ€. See naming note below.)

## Purpose
MVP HTML scraper for MDC pages. Joins in category labels and writes **lineâ€‘delimited JSON** records.

##Inputs / Outputs (defaults)
- In URLs: `../data/target_links_v3.txt`
- In labels: `../data/urls_with_category_v3.csv`
- Out JSONL: `../data/mdc_pages_v3.jsonl`

## Fetcher settings  
- Userâ€‘Agent: `"SkyyScraper/1.0 contact: lorenzo.garcet001@mymdc.net"`
- Timeout: 25s Â· Retries: 2 Â· Delay: 0.5â€“1.0s between requests
- Skips auth/portal frames (e.g., PeopleSoft, SSO) via regex patterns

## Extraction (per page)
- meta: `{ url, final_url, status_code, content_type, fetched_at }`
- Core: `title`, `meta_description`, `canonical_url`, `lang`, `meta_robots`, `meta_generator`
- Social: `og{title,description,type,url}`, `twitter{title,description,card}`
- Headings: arrays of `h1`, `h2`, `h3`
- Main text and word_count
- Contact facts: `emails`, `phones`, `money_amounts`, `dates` (regexâ€‘based)
- Structure: `breadcrumbs`, `structured_data_types` (from JSONâ€‘LD), and link summary:
  - `internal_count/external_count/pdf_count/...` plus small samples and basic CTA anchors
- Label join: `merges category`, `confidence`, `reason` if present
- Resumable: skips URLs already present in the output JSONL

## Error handling
- Nonâ€‘HTML or HTTP errors: writes a minimal record with meta only
- Parse errors: records parse_error

---
5) `scraper/pdf_scraper.py` â€” v1.0.0

# Purpose
Fetch PDFs, robustly resolve browser viewer pages to the actual PDF URL, extract text, and write **lineâ€‘delimited JSON.**

## Inputs / Outputs (defaults)
- In PDFs: `../data/pdf_links_v3.txt`
- In labels: `../data/pdfs_with_category_v3.csv` (optional)
- Out JSONL: `../data/mdc_pdfs_v3.jsonl`

## Fetcher settings
- Userâ€‘Agent: `"SkyyScraper-PDF/1.0 (+https://example.org) contact: you@example.org"` (placeholder in code; update if needed)
- Timeout: 30s Â· Delay: 0.5â€“1.0s Â· Streamed download with size cap (`--max_mb`, default 25 MB)

## Viewer/Wrapper resolution
- Detects `<embed original-url="...pdf">`, `<object data="...pdf">`, `<iframe src="...pdf">`, links to `.pdf`
- Handles `meta refresh` and a final regex scan for any `https://...pdf` in HTML

## Text extraction
- **pdfplumber** preferred; **PyPDF2** fallback
- **Extracted fields include:**
  - `url`, `final_url`, `status_code`, `content_type`, `fetched_at`
  - `file_size`, `file_sha256`, `page_count`, `word_count`
  - `text`, `plus derived emails`, `phones`, `money_amounts`, `dates`
  - `pdf_meta` (e.g., title/author/created/modified when available)
  - Category labels merged when provided

---
# ğŸ—‚ï¸ Current Data Artifacts (v3)

**From /data:**
- Discovery & cleaning
  - `mdc_links_raw_v3.txt` â€” all discovered links
  - `html_links_v3.txt` / `pdf_links_v3.txt` â€” cleaned, split
  - `mdc_crawler_errors_v3.log` â€” crawler issues
- Categorization
  - `urls_with_category_v3.csv` â€” HTML URLs with labels
  - `target_links_v3.txt` â€” HTML targets for MVP scraping
  - `pdfs_with_category_v3.csv` â€” labeled PDF URLs
- Scraped outputs
  - `mdc_pages_v3.jsonl` â€” HTML page records (lineâ€‘delimited JSON)
  - `mdc_pdfs_v3.jsonl` â€” PDF records (lineâ€‘delimited JSON)
`/aged_data_files` keeps v1/v2 era inputs/outputs for historical reference.

---
# ğŸ—“ï¸ Running Periodically
The pipeline is designed for scheduled runs (e.g., nightly/weekly):
- Reâ€‘run **crawler** â†’ **cleaner** â†’ **categorizer** to refresh target lists as the site changes.
- **Scrapers** are idempotent/resumable (HTML scraper skips URLs already present in the output). 

You can orchestrate this with your preferred scheduler (cron, task runner, CI), keeping the /data versioned (`v4`, `v5`, â€¦)
as needed.

---

